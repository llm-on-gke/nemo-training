_partial_: true
_target_: nemo.collections.llm.api.pretrain
data:
  _target_: nemo.collections.llm.gpt.data.mock.MockDataModule
  global_batch_size: 4096
  micro_batch_size: 1
  seq_length: 4096
log:
  _target_: nemo.lightning.nemo_logger.NeMoLogger
  ckpt: null
  explicit_log_dir: /gcs-dir/nemo-logs
  log_dir: null
  name: default
  tensorboard:
    _target_: lightning.pytorch.loggers.tensorboard.TensorBoardLogger
    name: default
    save_dir: tb_logs
  wandb: null
model:
  _target_: nemo.collections.llm.gpt.model.deepseek.DeepSeekModel
  config:
    _target_: nemo.collections.llm.gpt.model.deepseek.DeepSeekV3Config
    moe_layer_freq: &id001
    - 0
    - 1
    mtp_loss_scaling_factor: 0.1
    mtp_num_layers: 1
    num_layers: 4
    recompute_granularity: selective
    recompute_modules: &id002
    - mla_up_proj
    - layernorm
optim:
  _target_: nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule
  config:
    _target_: megatron.core.optimizer.optimizer_config.OptimizerConfig
    adam_beta1: 0.9
    adam_beta2: 0.95
    adam_eps: 1.0e-05
    bf16: true
    clip_grad: 1.0
    fp16: false
    lr: 0.0003
    optimizer: adam
    use_distributed_optimizer: true
    weight_decay: 0.1
  lr_scheduler:
    _target_: nemo.lightning.pytorch.optim.lr_scheduler.CosineAnnealingScheduler
    constant_steps: 0
    min_lr: 2.9999999999999997e-05
    warmup_steps: 2000
resume:
  _target_: nemo.lightning.resume.AutoResume
  resume_if_exists: true
  resume_ignore_no_checkpoint: true
trainer:
  _target_: nemo.lightning.pytorch.trainer.Trainer
  accelerator: gpu
  accumulate_grad_batches: 1
  callbacks:
  - _target_: nemo.utils.exp_manager.TimingCallback
  - _target_: nemo.lightning.pytorch.callbacks.deepep.DeepEPCallback
  - _target_: nemo.lightning.pytorch.callbacks.garbage_collection.GarbageCollectionCallback
    gc_interval_train: 5
    gc_interval_val: 5
  - _target_: nemo.lightning.pytorch.callbacks.megatron_comm_overlap.MegatronCommOverlapCallback
    tp_comm_overlap: false
  - _target_: nemo.lightning.pytorch.callbacks.garbage_collection.GarbageCollectionCallback
    gc_interval_train: 60
    gc_interval_val: 60
  - _target_: nemo.lightning.pytorch.callbacks.megatron_comm_overlap.MegatronCommOverlapCallback
    tp_comm_overlap: false
  - _target_: nemo.lightning.pytorch.callbacks.flops_callback.FLOPsMeasurementCallback
    data_config:
      _target_: nemo.collections.llm.gpt.data.mock.MockDataModule
      global_batch_size: 4096
      micro_batch_size: 1
      seq_length: 4096
    model_config:
      _target_: nemo.collections.llm.gpt.model.deepseek.DeepSeekV3Config
      moe_layer_freq: *id001
      mtp_loss_scaling_factor: 0.1
      mtp_num_layers: 1
      num_layers: 4
      recompute_granularity: selective
      recompute_modules: *id002
    model_name: deepseekv3
  devices: '8'
  enable_checkpointing: false
  limit_test_batches: 50
  limit_val_batches: 0.0
  log_every_n_steps: 1
  max_steps: 10
  num_nodes: 2
  num_sanity_val_steps: 0
  plugins:
    _target_: nemo.lightning.pytorch.plugins.mixed_precision.MegatronMixedPrecision
    autocast_enabled: false
    grad_reduce_in_fp32: false
    params_dtype:
      _call_: false
      _target_: torch.bfloat16
    pipeline_dtype:
      _call_: false
      _target_: torch.bfloat16
    precision: bf16-mixed
  strategy:
    _target_: nemo.lightning.pytorch.strategies.megatron_strategy.MegatronStrategy
    ckpt_async_save: true
    ckpt_parallel_load: true
    context_parallel_size: 1
    ddp:
      _target_: megatron.core.distributed.distributed_data_parallel_config.DistributedDataParallelConfig
      average_in_collective: true
      check_for_nan_in_grad: true
      grad_reduce_in_fp32: false
      overlap_grad_reduce: true
      overlap_param_gather: true
    expert_model_parallel_size: 8
    expert_tensor_parallel_size: 1
    gradient_as_bucket_view: true
    num_layers_in_first_pipeline_stage: 3
    num_layers_in_last_pipeline_stage: 2
    pipeline_dtype:
      _call_: false
      _target_: torch.bfloat16
    pipeline_model_parallel_size: 1
    sequence_parallel: true
    tensor_model_parallel_size: 8
    virtual_pipeline_model_parallel_size: null
  use_distributed_sampler: false
  val_check_interval: 100