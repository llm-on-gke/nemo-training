# Distributed training of a traditional CNN model to do image classification 
# using the MNIST dataset and PyTorch.
apiVersion: jobset.x-k8s.io/v1alpha2
kind: JobSet
metadata:
  name: deepseek
  labels:
      kueue.x-k8s.io/queue-name: a3-mega
  #annotations:
  #     provreq.kueue.x-k8s.io/maxRunDurationDays: "7"
  #End for dws
spec:
  replicatedJobs:
  - name: workload
    template:
      spec:
        parallelism: 32
        completions: 32
        # suspend: true # DWS
        backoffLimit: 0
        template:
          metadata:
              annotations:
                gke-gcsfuse/volumes: "true"
                kueue.x-k8s.io/podset-preferred-topology: "kubernetes.io/hostname"
          spec:
            #schedulingGates:
            #- name: "gke.io/topology-aware-auto-scheduling-pytorch"
            restartPolicy: Never # dws
            hostNetwork: true
            dnsPolicy: ClusterFirstWithHostNet
            nodeSelector:
              # cloud.google.com/gke-nodepool: dws-a3-mega #for dws
              cloud.google.com/gke-accelerator: nvidia-h100-mega-80gb #for non dws
            tolerations:
               - key: "nvidia.com/gpu"
                 operator: "Exists"
                 effect: "NoSchedule"
            serviceAccountName: storage-access
            volumes:
            - name: libraries
              hostPath:
                path: /home/kubernetes/bin/nvidia
            - name: sys
              hostPath:
               path: /sys
            - name: proc-sys
              hostPath:
               path: /proc/sys
            - name: aperture-devices
              hostPath:
               path: /dev/aperture_devices
            - name: local-ssd
              hostPath:
                path: /mnt/stateful_partition/kube-ephemeral-ssd
            - name: var-lib
              hostPath:
                path: /var/lib
            - name: nccl-plugin-volume
              emptyDir: {}

            - name: gcs-fuse-csi-ephemeral
              csi:
                 driver: gcsfuse.csi.storage.gke.io
                 readOnly: false
                 volumeAttributes:
                    bucketName: rick-nemo-factory
                    mountOptions: "implicit-dirs"
                    gcsfuseLoggingSeverity: warning
            #        fileCacheCapacity: "200Gi"

            - name: dshm
              emptyDir:
                medium: Memory
            #- name: workload-configuration
            #  configMap:
            #    name: "workload-config"
            #    items:
            #    - key: workload-configuration
            #      path: workload-configuration

            #- name: workload-launcher
             # configMap:
             #   name: "workload-launcher"
            initContainers:
            - name: tcpxo-daemon
              image: us-docker.pkg.dev/gce-ai-infra/gpudirect-tcpxo/tcpgpudmarxd-dev:v1.0.14
              imagePullPolicy: IfNotPresent
              restartPolicy: Always
              securityContext:
                privileged: true
                
              volumeMounts:
               - name: libraries
                 mountPath: /usr/local/nvidia
               - name: sys
                 mountPath: /hostsysfs
               - name: proc-sys
                 mountPath: /hostprocsysfs
              env:
                - name: LD_LIBRARY_PATH
                  value: "/usr/local/nvidia/lib64"
            - name: nccl-tcpxo-installer
              image: us-docker.pkg.dev/gce-ai-infra/gpudirect-tcpxo/nccl-plugin-gpudirecttcpx-dev:v1.0.8-1 #v1.0.11
              imagePullPolicy: IfNotPresent
              securityContext:
                privileged: true
              volumeMounts:
              - name: var-lib
                mountPath: /var/lib
              - name: libraries
                mountPath: /usr/local/nvidia
              - name: nccl-plugin-volume
                mountPath: /usr/local/nccl-plugin
              command: ["/bin/sh", "-c"]
              args:
               - |
                 set -ex
                 chmod 755 /scripts/container_entry.sh
                 rm -r -f /var/lib/tcpxo
                 /scripts/container_entry.sh install --install-nccl --nccl-buildtype=223
                 cp -r /var/lib/tcpxo/* /usr/local/nccl-plugin/
                 echo "Installed NCCL plugin to pod-wide, shared NCCL plug-in volume"
                 echo "Contents (mounted at /usr/local/nccl-plugin/lib64):"
                 ls /usr/local/nccl-plugin/lib64 | sed 's/^/  /'
                 echo "Contents (mounted at /usr/local/nccl-plugin/):"
                 ls /usr/local/nccl-plugin/ | sed 's/^/  /'
            containers:
            - name: pytorch
              image: nvcr.io/nvidia/nemo:25.07
              imagePullPolicy: IfNotPresent
              # image: gcr.io/k8s-staging-jobset/pytorch-mnist:latest
              ports:
              - containerPort: 3389
              env:
              - name: HF_TOKEN
                valueFrom:
                  secretKeyRef:
                    name: hf-secret
                    key: hf_api_token
              - name: HUGGING_FACE_HUB_TOKEN
                valueFrom:
                  secretKeyRef:
                    name: hf-secret
                    key: hf_api_token
              - name: JOB_IDENTIFIER
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.annotations['jobset.sigs.k8s.io/jobset-name']
              - name: JOB_ORCHESTRATOR
                value: "gke"
              # Add RANK based on the pod's index provided by the Indexed Job
              # This is crucial for torch.distributed initialization.
              - name: JOB_COMPLETION_INDEX
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']
              - name: REPLICATED_JOB_NAME
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.annotations['jobset.sigs.k8s.io/replicatedjob-name']
              - name: JOBSET_NAME
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.annotations['jobset.sigs.k8s.io/jobset-name']
              - name: MASTER_ADDR
                value: "deepseek-workload-0-0.deepseek.default.svc.cluster.local"
              - name: MASTER_PORT
                value: "3389"
              - name: MAX_STEPS
                value: "20"
              - name: NNODES
                value: "32"
              - name: GPUS_PER_NODE
                value: "8"
              - name: EXPERIMENT_NAME
                value: "nemo-experiments"
              - name: EXPERIMENT_ROOT_DIR
                value: "/gcs-dir/"
              - name: EXPLICIT_LOG_DIR
                value: "/gcs-dir/nemo-logs"
              - name: NVTE_FWD_LAYERNORM_SM_MARGIN
                value: "8"
              - name: NVTE_BWD_LAYERNORM_SM_MARGIN
                value: "8"
              - name: CUDA_DEVICE_MAX_CONNECTIONS
                value: "1"
              
              - name: GLOO_SOCKET_IFNAME
                value: "eth0"
              - name: TOKENIZER_PATH
                value: "/gcs-dir/nemo-configs"
              - name: NEMO_LAUNCH_SCRIPT
                value: /gcs-dir/nemo-configs/deepseek-v3.py
                #value: /gcs-dir/nemo-configs/llama3-405b.py
              
  # Force python to not buffer output and write directly to stdout, so we can view training logs via `kubectl logs`.
              - name: PYTHONUNBUFFERED
                value: "0"
        
              - name: NCCL_LIB_DIR
                value: /usr/local/nccl-plugin/lib64 #/usr/local/nccl-plugin/lib64
              - name: NCCL_PLUGIN_PATH
                value: /usr/local/nccl-plugin/lib64 #/usr/local/nccl-plugin/lib64 #:
              - name: NCCL_INIT_SCRIPT                   
                value: /usr/local/nccl-plugin/lib64/nccl-env-profile.sh
              - name: NCCL_FASTRAK_LLCM_DEVICE_DIRECTORY
                value: "/dev/aperture_devices"

              
              securityContext:
                privileged: true
              command:
              - bash
              - -xc
              - |
                trap "" SIGPROF
                echo "Pod on $(hostname --fqdn) is running"
                echo "Pod is assigned job index of $JOB_COMPLETION_INDEX"
      
                if [[ -n "${NCCL_INIT_SCRIPT}" ]]; then
                   echo "Running NCCL init script: ${NCCL_INIT_SCRIPT}"
                   source ${NCCL_INIT_SCRIPT}
                fi
                
                export LD_LIBRARY_PATH="$NCCL_PLUGIN_PATH"
                ldconfig $LD_LIBRARY_PATH
                #echo "Added $LD_LIBRARY_PATH to ldconfig:"
                ldconfig -p | grep libcuda | sed 's/^/  /'
                echo ""

                #NCCL_DEBUG="INFO"
                #sleep infinity
                mkdir /tb_boards
                bash /gcs-dir/nemo-configs/nemo-2-launcher.sh

              resources:
                requests:
                  #cpu: "8"
                  #memory: "25Gi"
                  #ephemeral-storage: "25Gi"
                  nvidia.com/gpu: 8
                limits:
                  # cpu: "16"
                  # memory: "30Gi"
                  # ephemeral-storage: "30Gi"
                  nvidia.com/gpu: 8
              volumeMounts:
               - name: aperture-devices
                 mountPath: /dev/aperture_devices
               - name: libraries
                 mountPath: /usr/local/nvidia
               - name: nccl-plugin-volume
                 mountPath: /usr/local/nccl-plugin
               - mountPath: /dev/shm
                 name: dshm
               - mountPath: /gcs-dir
                 name: gcs-fuse-csi-ephemeral
               - name: local-ssd
                 mountPath: /localssd

               #- name: hf-cache
               #  mountPath: /root/.cache/huggingface
              
               #- name: workload-configuration
               #  mountPath: /workload/configs

               #- name: workload-launcher
               #  mountPath: /workload/launcher
